{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-86da63a3aaf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "from keras.models import load_model, model_from_json\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'C:/Users/User-PC/Mubs/Datascience/mlab/aws/650_1/'\n",
    "weights = \"app/data/model/weights/org-weights-epoch-0060--loss-0.0261.hdf5\"\n",
    "model_json = 'app/data/model/model.json'\n",
    "bootstrap_vocab = 'app/data/model/bootstrap.vocab'\n",
    "web_dsl_mapping = \"app/data/model/web-dsl-mapping.json\"\n",
    "\n",
    "# Read a file and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images = []\n",
    "    # Load all the files and order them\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames)[-6:]:\n",
    "        if filename[-3:] == \"npz\":\n",
    "            # Load the images already prepared in arrays\n",
    "            image = np.load(data_dir+filename)\n",
    "            images.append(image['features'])\n",
    "        else:\n",
    "            # Load the boostrap tokens and rap them in a start and end tag\n",
    "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
    "            # Seperate all the words with a single space\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            # Add a space after each comma\n",
    "            # syntax = syntax.replace(',', ' ,')\n",
    "            text.append(syntax)\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the function to create the vocabulary \n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "# Create the vocabulary \n",
    "tokenizer.fit_on_texts([load_doc(bootstrap_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = dataset\n",
    "train_features, texts = load_data(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load model and weights\n",
    "json_file = open(model_json, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(weights)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    photo = np.array([photo])\n",
    "    # seed the generation process\n",
    "    in_text = '<START> '\n",
    "    # iterate over the whole length of the sequence\n",
    "    print('\\nPrediction---->\\n\\n<START> ', end='')\n",
    "    for i in range(750):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += word + ' '\n",
    "        # stop if we predict the end of the sequence\n",
    "        print(word + ' ', end='')\n",
    "        if word == '<END>':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 48 \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for i in range(len(texts)):\n",
    "        yhat = generate_desc(loaded_model, tokenizer, photos[i], max_length)\n",
    "        # store actual and predicted\n",
    "        print('\\n\\nReal---->\\n\\n' + texts[i])\n",
    "        actual.append([texts[i].split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu, actual, predicted\n",
    "\n",
    "bleu, actual, predicted = evaluate_model(loaded_model, texts, train_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the tokens into HTML and css\n",
    "dsl_path = web_dsl_mapping\n",
    "class Node:\n",
    "    def __init__(self, key, parent_node, content_holder):\n",
    "        self.key = key\n",
    "        if (self.key) == \"\":\n",
    "            self.key = \"line\"\n",
    "            print(\"Empty token found\")\n",
    "        self.parent = parent_node\n",
    "        self.children = []\n",
    "        self.content_holder = content_holder\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "        # if \"{}\" in child:\n",
    "        #     print(\"has parentheses\")\n",
    "        # else:\n",
    "        #     print(\"no parantheses\")\n",
    "\n",
    "    def show(self):\n",
    "        for child in self.children:\n",
    "            child.show()\n",
    "\n",
    "    def render(self, mapping, rendering_function=None):\n",
    "        content = \"body\"\n",
    "        for child in self.children:\n",
    "            placeholder = child.render(mapping, rendering_function)\n",
    "            # else add none\n",
    "            if placeholder is None:\n",
    "                self = None\n",
    "                return\n",
    "            else:\n",
    "                content += placeholder\n",
    "\n",
    "        # print(\"self.key:\", self.key)\n",
    "        value = mapping.get(self.key, None)\n",
    "        # print(\"Value mapping.get(self.key, None)\", value)\n",
    "        if value is None:\n",
    "            self = None\n",
    "            return None\n",
    "        value = rendering_function(self.key, value)\n",
    "        # print(\"value: \\n\", value)\n",
    "\n",
    "        if len(self.children) != 0:\n",
    "            value = value.replace(self.content_holder, content)\n",
    "            # print(\"value rendering_function: \\n\", value)\n",
    "        return value\n",
    "\n",
    "class Utils:\n",
    "    @staticmethod\n",
    "    def get_random_text(length_text=10, space_number=1, with_upper_case=True):\n",
    "        results = []\n",
    "        while len(results) < length_text:\n",
    "            char = random.choice(string.ascii_letters[:26])\n",
    "            results.append(char)\n",
    "        if with_upper_case:\n",
    "            results[0] = results[0].upper()\n",
    "\n",
    "        current_spaces = []\n",
    "        while len(current_spaces) < space_number:\n",
    "            space_pos = random.randint(2, length_text - 3)\n",
    "            if space_pos in current_spaces:\n",
    "                break\n",
    "            results[space_pos] = \" \"\n",
    "            if with_upper_case:\n",
    "                results[space_pos + 1] = results[space_pos - 1].upper()\n",
    "\n",
    "            current_spaces.append(space_pos)\n",
    "\n",
    "        return ''.join(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ios_id(length=10):\n",
    "        results = []\n",
    "\n",
    "        while len(results) < length:\n",
    "            char = random.choice(string.digits + string.ascii_letters)\n",
    "            results.append(char)\n",
    "\n",
    "        results[3] = \"-\"\n",
    "        results[6] = \"-\"\n",
    "\n",
    "        return ''.join(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_android_id(length=10):\n",
    "        results = []\n",
    "\n",
    "        while len(results) < length:\n",
    "            char = random.choice(string.ascii_letters)\n",
    "            results.append(char)\n",
    "\n",
    "        return ''.join(results)\n",
    "\n",
    "def render_content_with_text(key, value):\n",
    "    if FILL_WITH_RANDOM_TEXT:\n",
    "        if key.find(\"button\") != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, Utils.get_random_text())\n",
    "        elif key.find(\"title\") != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, Utils.get_random_text(length_text=5, space_number=0))\n",
    "        elif key.find(\"text\") != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER,\n",
    "                                  Utils.get_random_text(length_text=56, space_number=7, with_upper_case=False))\n",
    "    return value\n",
    "        \n",
    "class Compiler:\n",
    "    def __init__(self, dsl_mapping_file_path):\n",
    "        with open(dsl_mapping_file_path) as data_file:\n",
    "            self.dsl_mapping = json.load(data_file)\n",
    "\n",
    "        self.opening_tag = self.dsl_mapping[\"opening-tag\"]\n",
    "        self.closing_tag = self.dsl_mapping[\"closing-tag\"]\n",
    "        self.content_holder = self.opening_tag + self.closing_tag\n",
    "\n",
    "        self.root = Node(\"\", None, self.content_holder)\n",
    "\n",
    "    def compile(self, tokens, output_file_path):\n",
    "        dsl_file = tokens\n",
    "        # Parse fix\n",
    "        dsl_file = dsl_file[1:-1]\n",
    "        dsl_file = ' '.join(dsl_file)\n",
    "        dsl_file = dsl_file.replace('{', '{888').replace('}', '888}888')\n",
    "        dsl_file = dsl_file.replace(' ', '')\n",
    "        dsl_file = dsl_file.replace(',', '888')\n",
    "        dsl_file = dsl_file.split('888')\n",
    "        # print(\"dsl_file\", dsl_file)\n",
    "        dsl_file = list(filter(None, dsl_file))\n",
    "        # End Parse fix\n",
    "        current_parent = self.root\n",
    "        for token in dsl_file:\n",
    "            token = token.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "            if token.find(self.opening_tag) != -1:\n",
    "                token = token.replace(self.opening_tag, \"\")\n",
    "                # print(\"token\", token)\n",
    "                element = Node(token, current_parent, self.content_holder)\n",
    "                # print(\"element\", element)\n",
    "                current_parent.add_child(element)\n",
    "                current_parent = element\n",
    "\n",
    "            elif token.find(self.closing_tag) != -1:\n",
    "                current_parent = current_parent.parent\n",
    "            else:\n",
    "                tokens = token.split(\",\")\n",
    "                for t in tokens:\n",
    "                    element = Node(t, current_parent, self.content_holder)\n",
    "                    current_parent.add_child(element)\n",
    "\n",
    "        # print('self.root.render(self.dsl_mapping', self.root.render(self.dsl_mapping))\n",
    "        output_html = self.root.render(self.dsl_mapping, rendering_function=render_content_with_text)\n",
    "\n",
    "        # print(\"output_html:\", output_html)\n",
    "\n",
    "        if output_html is None:\n",
    "            return \"Parsing Error\"\n",
    "\n",
    "        print(\"Compiling website\")\n",
    "\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(output_html)\n",
    "        return output_html\n",
    "\n",
    "FILL_WITH_RANDOM_TEXT = True\n",
    "TEXT_PLACE_HOLDER = \"[]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compiler = Compiler(dsl_path)\n",
    "compiled_website = compiler.compile(predicted[0], 'index.html')\n",
    "print(compiled_website )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bleu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
